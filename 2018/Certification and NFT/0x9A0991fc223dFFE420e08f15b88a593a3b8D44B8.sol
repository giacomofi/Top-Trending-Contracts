['pragma solidity ^0.4.19;\n', '// Danku contract version 0.0.1\n', '// Data points are x, y, and z\n', '\n', 'contract Danku_demo {\n', '  function Danku_demo() public {\n', '    // Neural Network Structure:\n', '    //\n', '    // (assertd) input layer x number of neurons\n', '    // (optional) hidden layers x number of neurons\n', '    // (assertd) output layer x number of neurons\n', '  }\n', '  struct Submission {\n', '      address payment_address;\n', '      // Define the number of neurons each layer has.\n', '      uint num_neurons_input_layer;\n', '      uint num_neurons_output_layer;\n', '      // There can be multiple hidden layers.\n', '      uint[] num_neurons_hidden_layer;\n', '      // Weights indexes are the following:\n', '      // weights[l_i x l_n_i x pl_n_i]\n', '      // Also number of layers in weights is layers.length-1\n', '      int256[] weights;\n', '      int256[] biases;\n', '  }\n', '  struct NeuralLayer {\n', '    int256[] neurons;\n', '    int256[] errors;\n', '    string layer_type;\n', '  }\n', '\n', '  address public organizer;\n', '  // Keep track of the best model\n', '  uint public best_submission_index;\n', '  // Keep track of best model accuracy\n', '  int256 public best_submission_accuracy = 0;\n', '  // The model accuracy criteria\n', '  int256 public model_accuracy_criteria;\n', '  // Use test data if provided\n', '  bool public use_test_data = false;\n', '  // Each partition is 5% of the total dataset size\n', '  uint constant partition_size = 25;\n', '  // Data points are made up of x and y coordinates and the prediction\n', '  uint constant datapoint_size = 3;\n', '  uint constant prediction_size = 1;\n', '  // Max number of data groups\n', '  // Change this to your data group size\n', '  uint16 constant max_num_data_groups = 500;\n', '  // Training partition size\n', '  uint16 constant training_data_group_size = 400;\n', '  // Testing partition size\n', '  uint16 constant testing_data_group_size = max_num_data_groups - training_data_group_size;\n', '  // Dataset is divided into data groups.\n', '  // Every data group includes a nonce.\n', '  // Look at sha_data_group() for more detail about hashing a data group\n', '  bytes32[max_num_data_groups/partition_size] hashed_data_groups;\n', '  // Nonces are revelead together with data groups\n', '  uint[max_num_data_groups/partition_size] data_group_nonces;\n', '  // + 1 for prediction\n', '  // A data group has 3 data points in total\n', '  int256[datapoint_size][] public train_data;\n', '  int256[datapoint_size][] public test_data;\n', '  bytes32 partition_seed;\n', '  // Deadline for submitting solutions in terms of block size\n', '  uint public submission_stage_block_size = 241920; // 6 weeks timeframe\n', '  // Deadline for revealing the testing dataset\n', '  uint public reveal_test_data_groups_block_size = 17280; // 3 days timeframe\n', '  // Deadline for evaluating the submissions\n', '  uint public evaluation_stage_block_size = 40320; // 7 days timeframe\n', '  uint public init1_block_height;\n', '  uint public init3_block_height;\n', '  uint public init_level = 0;\n', '  // Training partition size is 14 (70%)\n', '  // Testing partition size is 6 (30%)\n', '  uint[training_data_group_size/partition_size] public training_partition;\n', '  uint[testing_data_group_size/partition_size] public testing_partition;\n', '  uint256 train_dg_revealed = 0;\n', '  uint256 test_dg_revealed = 0;\n', '  Submission[] submission_queue;\n', '  bool public contract_terminated = false;\n', '  // Integer precision for calculating float values for weights and biases\n', '  int constant int_precision = 10000;\n', '\n', '  // Takes in array of hashed data points of the entire dataset,\n', '  // submission and evaluation times\n', '  function init1(bytes32[max_num_data_groups/partition_size] _hashed_data_groups, int accuracy_criteria, address organizer_refund_address) external {\n', '    // Make sure contract is not terminated\n', '    assert(contract_terminated == false);\n', '    // Make sure it&#39;s called in order\n', '    assert(init_level == 0);\n', '    organizer = organizer_refund_address;\n', '    init_level = 1;\n', '    init1_block_height = block.number;\n', '\n', '    // Make sure there are in total 20 hashed data groups\n', '    assert(_hashed_data_groups.length == max_num_data_groups/partition_size);\n', '    hashed_data_groups = _hashed_data_groups;\n', '    // Accuracy criteria example: 85.9% => 8,590\n', '    // 100 % => 10,000\n', '    assert(accuracy_criteria > 0);\n', '    model_accuracy_criteria = accuracy_criteria;\n', '  }\n', '\n', '  function init2() external {\n', '    // Make sure contract is not terminated\n', '    assert(contract_terminated == false);\n', '    // Only allow calling it once, in order\n', '    assert(init_level == 1);\n', '    // Make sure it&#39;s being called within 20 blocks on init1()\n', '    // to minimize organizer influence on random index selection\n', '    if (block.number <= init1_block_height+20 && block.number > init1_block_height) {\n', '      // TODO: Also make sure it&#39;s being called 1 block after init1()\n', '      // Randomly select indexes\n', '      uint[] memory index_array = new uint[](max_num_data_groups/partition_size);\n', '      for (uint i = 0; i < max_num_data_groups/partition_size; i++) {\n', '        index_array[i] = i;\n', '      }\n', '      randomly_select_index(index_array);\n', '      init_level = 2;\n', '    } else {\n', '      // Cancel the contract if init2() hasn&#39;t been called within 5\n', '      // blocks of init1()\n', '      cancel_contract();\n', '    }\n', '  }\n', '\n', '  function init3(int256[] _train_data_groups, int256 _train_data_group_nonces) external {\n', '    // Pass a single data group at a time\n', '    // Make sure contract is not terminated\n', '    assert(contract_terminated == false);\n', '    // Only allow calling once, in order\n', '    assert(init_level == 2);\n', '    // Verify data group and nonce lengths\n', '    assert((_train_data_groups.length/partition_size)/datapoint_size == 1);\n', '    // Verify data group hashes\n', '    // Order of revealed training data group must be the same with training partitions\n', '    // Otherwise hash verification will fail\n', '    assert(sha_data_group(_train_data_groups, _train_data_group_nonces) ==\n', '      hashed_data_groups[training_partition[train_dg_revealed]]);\n', '    train_dg_revealed += 1;\n', '    // Assign training data after verifying the corresponding hash\n', '    unpack_data_groups(_train_data_groups, true);\n', '    if (train_dg_revealed == (training_data_group_size/partition_size)) {\n', '      init_level = 3;\n', '      init3_block_height = block.number;\n', '    }\n', '  }\n', '\n', '  function get_training_index() public view returns(uint[training_data_group_size/partition_size]) {\n', '    return training_partition;\n', '  }\n', '\n', '  function get_testing_index() public view returns(uint[testing_data_group_size/partition_size]) {\n', '    return testing_partition;\n', '  }\n', '\n', '  function get_submission_queue_length() public view returns(uint) {\n', '    return submission_queue.length;\n', '  }\n', '\n', '  function submit_model(\n', '    // Public function for users to submit a solution\n', '    address payment_address,\n', '    uint num_neurons_input_layer,\n', '    uint num_neurons_output_layer,\n', '    uint[] num_neurons_hidden_layer,\n', '    int[] weights,\n', '    int256[] biases) public {\n', '      // Make sure contract is not terminated\n', '      assert(contract_terminated == false);\n', '      // Make sure it&#39;s not the initialization stage anymore\n', '      assert(init_level == 3);\n', '      // Make sure it&#39;s still within the submission stage\n', '      assert(block.number < init3_block_height + submission_stage_block_size);\n', '      // Make sure that num of neurons in the input & output layer matches\n', '      // the problem description\n', '      assert(num_neurons_input_layer == datapoint_size - prediction_size);\n', '      // Because we can encode binary output in two different ways, we check\n', '      // for both of them\n', '      assert(num_neurons_output_layer == prediction_size || num_neurons_output_layer == (prediction_size+1));\n', '      // Make sure that the number of weights match network structure\n', '      assert(valid_weights(weights, num_neurons_input_layer, num_neurons_output_layer, num_neurons_hidden_layer));\n', '      // Add solution to submission queue\n', '      submission_queue.push(Submission(\n', '        payment_address,\n', '        num_neurons_input_layer,\n', '        num_neurons_output_layer,\n', '        num_neurons_hidden_layer,\n', '        weights,\n', '        biases));\n', '  }\n', '\n', '  function get_submission_id(\n', '    // Public function that returns the submission index ID\n', '    address paymentAddress,\n', '    uint num_neurons_input_layer,\n', '    uint num_neurons_output_layer,\n', '    uint[] num_neurons_hidden_layer,\n', '    int[] weights,\n', '    int256[] biases) public view returns (uint) {\n', '      // Iterate over submission queue to get submission index ID\n', '      for (uint i = 0; i < submission_queue.length; i++) {\n', '        if (submission_queue[i].payment_address != paymentAddress) {\n', '          continue;\n', '        }\n', '        if (submission_queue[i].num_neurons_input_layer != num_neurons_input_layer) {\n', '          continue;\n', '        }\n', '        if (submission_queue[i].num_neurons_output_layer != num_neurons_output_layer) {\n', '          continue;\n', '        }\n', '        for (uint j = 0; j < num_neurons_hidden_layer.length; j++) {\n', '            if (submission_queue[i].num_neurons_hidden_layer[j] != num_neurons_hidden_layer[j]) {\n', '              continue;\n', '            }\n', '        }\n', '        for (uint k = 0; k < weights.length; k++) {\n', '            if (submission_queue[i].weights[k] != weights[k]) {\n', '              continue;\n', '            }\n', '        }\n', '        for (uint l = 0; l < biases.length; l++) {\n', '          if (submission_queue[i].biases[l] != biases[l]) {\n', '            continue;\n', '          }\n', '        }\n', '        // If everything matches, return the submission index\n', '        return i;\n', '      }\n', '      // If submission is not in the queue, just throw an exception\n', '      require(false);\n', '  }\n', '\n', '    function reveal_test_data(int256[] _test_data_groups, int256 _test_data_group_nonces) external {\n', '    // Make sure contract is not terminated\n', '    assert(contract_terminated == false);\n', '    // Make sure it&#39;s not the initialization stage anymore\n', '    assert(init_level == 3);\n', '    // Make sure it&#39;s revealed after the submission stage\n', '    assert(block.number >= init3_block_height + submission_stage_block_size);\n', '    // Make sure it&#39;s revealed within the reveal stage\n', '    assert(block.number < init3_block_height + submission_stage_block_size + reveal_test_data_groups_block_size);\n', '    // Verify data group and nonce lengths\n', '    assert((_test_data_groups.length/partition_size)/datapoint_size == 1);\n', '    // Verify data group hashes\n', '    assert(sha_data_group(_test_data_groups, _test_data_group_nonces) ==\n', '      hashed_data_groups[testing_partition[test_dg_revealed]]);\n', '    test_dg_revealed += 1;\n', '    // Assign testing data after verifying the corresponding hash\n', '    unpack_data_groups(_test_data_groups, false);\n', '    // Use test data for evaluation\n', '    use_test_data = true;\n', '  }\n', '\n', '  function evaluate_model(uint submission_index) public {\n', '    // TODO: Make sure that if there&#39;s two same submission w/ same weights\n', '    // and biases, the first one submitted should get the reward.\n', '    // Make sure contract is not terminated\n', '    assert(contract_terminated == false);\n', '    // Make sure it&#39;s not the initialization stage anymore\n', '    assert(init_level == 3);\n', '    // Make sure it&#39;s evaluated after the reveal stage\n', '    assert(block.number >= init3_block_height + submission_stage_block_size + reveal_test_data_groups_block_size);\n', '    // Make sure it&#39;s evaluated within the evaluation stage\n', '    assert(block.number < init3_block_height + submission_stage_block_size + reveal_test_data_groups_block_size + evaluation_stage_block_size);\n', '    // Evaluates a submitted model & keeps track of the best model\n', '    int256 submission_accuracy = 0;\n', '    if (use_test_data == true) {\n', '      submission_accuracy = model_accuracy(submission_index, test_data);\n', '    } else {\n', '      submission_accuracy = model_accuracy(submission_index, train_data);\n', '    }\n', '\n', '    // Keep track of the most accurate model\n', '    if (submission_accuracy > best_submission_accuracy) {\n', '      best_submission_index = submission_index;\n', '      best_submission_accuracy = submission_accuracy;\n', '    }\n', '  }\n', '\n', '  function cancel_contract() public {\n', '    // Make sure contract is not already terminated\n', '    assert(contract_terminated == false);\n', '    // Contract can only be cancelled if initialization has failed.\n', '    assert(init_level < 3);\n', '    // Refund remaining balance to organizer\n', '    organizer.transfer(this.balance);\n', '    // Terminate contract\n', '    contract_terminated = true;\n', '  }\n', '\n', '  function finalize_contract() public {\n', '    // Make sure contract is not terminated\n', '    assert(contract_terminated == false);\n', '    // Make sure it&#39;s not the initialization stage anymore\n', '    assert(init_level == 3);\n', '    // Make sure the contract is finalized after the evaluation stage\n', '    assert(block.number >= init3_block_height + submission_stage_block_size + reveal_test_data_groups_block_size + evaluation_stage_block_size);\n', '    // Get the best submission to compare it against the criteria\n', '    Submission memory best_submission = submission_queue[best_submission_index];\n', '    // If best submission passes criteria, payout to the submitter\n', '    if (best_submission_accuracy >= model_accuracy_criteria) {\n', '      best_submission.payment_address.transfer(this.balance);\n', '    // If the best submission fails the criteria, refund the balance back to the organizer\n', '    } else {\n', '      organizer.transfer(this.balance);\n', '    }\n', '    contract_terminated = true;\n', '  }\n', '\n', '  function model_accuracy(uint submission_index, int256[datapoint_size][] data) public constant returns (int256){\n', '    // Make sure contract is not terminated\n', '    assert(contract_terminated == false);\n', '    // Make sure it&#39;s not the initialization stage anymore\n', '    assert(init_level == 3);\n', '    // Leave function public for offline error calculation\n', '    // Get&#39;s the sum error for the model\n', '    Submission memory sub = submission_queue[submission_index];\n', '    int256 true_prediction = 0;\n', '    int256 false_prediction = 0;\n', '    bool one_hot; // one-hot encoding if prediction size is 1 but model output size is 2\n', '    int[] memory prediction;\n', '    int[] memory ground_truth;\n', '    if ((prediction_size + 1) == sub.num_neurons_output_layer) {\n', '      one_hot = true;\n', '      prediction = new int[](sub.num_neurons_output_layer);\n', '      ground_truth = new int[](sub.num_neurons_output_layer);\n', '    } else {\n', '      one_hot = false;\n', '      prediction = new int[](prediction_size);\n', '      ground_truth = new int[](prediction_size);\n', '    }\n', '    for (uint i = 0; i < data.length; i++) {\n', '      // Get ground truth\n', '      for (uint j = datapoint_size-prediction_size; j < data[i].length; j++) {\n', '        uint d_index = j - datapoint_size + prediction_size;\n', '        // Only get prediction values\n', '        if (one_hot == true) {\n', '          if (data[i][j] == 0) {\n', '            ground_truth[d_index] = 1;\n', '            ground_truth[d_index + 1] = 0;\n', '          } else if (data[i][j] == 1) {\n', '            ground_truth[d_index] = 0;\n', '            ground_truth[d_index + 1] = 1;\n', '          } else {\n', '            // One-hot encoding for more than 2 classes is not supported\n', '            require(false);\n', '          }\n', '        } else {\n', '          ground_truth[d_index] = data[i][j];\n', '        }\n', '      }\n', '      // Get prediction\n', '      prediction = get_prediction(sub, data[i]);\n', '      // Get error for the output layer\n', '      for (uint k = 0; k < ground_truth.length; k++) {\n', '        if (ground_truth[k] == prediction[k]) {\n', '          true_prediction += 1;\n', '        } else {\n', '          false_prediction += 1;\n', '        }\n', '      }\n', '    }\n', '    // We multipl by int_precision to get up to x decimal point precision while\n', '    // calculating the accuracy\n', '    return (true_prediction * int_precision) / (true_prediction + false_prediction);\n', '  }\n', '\n', '  function get_train_data_length() public view returns(uint256) {\n', '    return train_data.length;\n', '  }\n', '\n', '  function get_test_data_length() public view returns(uint256) {\n', '    return test_data.length;\n', '  }\n', '\n', '  function round_up_division(int256 dividend, int256 divisor) private pure returns(int256) {\n', '    // A special trick since solidity normall rounds it down\n', '    return (dividend + divisor -1) / divisor;\n', '  }\n', '\n', '  function not_in_train_partition(uint[training_data_group_size/partition_size] partition, uint number) private pure returns (bool) {\n', '    for (uint i = 0; i < partition.length; i++) {\n', '      if (number == partition[i]) {\n', '        return false;\n', '      }\n', '    }\n', '    return true;\n', '  }\n', '\n', '  function randomly_select_index(uint[] array) private {\n', '    uint t_index = 0;\n', '    uint array_length = array.length;\n', '    uint block_i = 0;\n', '    // Randomly select training indexes\n', '    while(t_index < training_partition.length) {\n', '      uint random_index = uint(sha256(block.blockhash(block.number-block_i))) % array_length;\n', '      training_partition[t_index] = array[random_index];\n', '      array[random_index] = array[array_length-1];\n', '      array_length--;\n', '      block_i++;\n', '      t_index++;\n', '    }\n', '    t_index = 0;\n', '    while(t_index < testing_partition.length) {\n', '      testing_partition[t_index] = array[array_length-1];\n', '      array_length--;\n', '      t_index++;\n', '    }\n', '  }\n', '\n', '  function valid_weights(int[] weights, uint num_neurons_input_layer, uint num_neurons_output_layer, uint[] num_neurons_hidden_layer) private pure returns (bool) {\n', '    // make sure the number of weights match the network structure\n', '    // get number of weights based on network structure\n', '    uint ns_total = 0;\n', '    uint wa_total = 0;\n', '    uint number_of_layers = 2 + num_neurons_hidden_layer.length;\n', '\n', '    if (number_of_layers == 2) {\n', '      ns_total = num_neurons_input_layer * num_neurons_output_layer;\n', '    } else {\n', '      for(uint i = 0; i < num_neurons_hidden_layer.length; i++) {\n', '        // Get weights between first hidden layer and input layer\n', '        if (i==0){\n', '          ns_total += num_neurons_input_layer * num_neurons_hidden_layer[i];\n', '        // Get weights between hidden layers\n', '        } else {\n', '          ns_total += num_neurons_hidden_layer[i-1] * num_neurons_hidden_layer[i];\n', '        }\n', '      }\n', '      // Get weights between last hidden layer and output layer\n', '      ns_total += num_neurons_hidden_layer[num_neurons_hidden_layer.length-1] * num_neurons_output_layer;\n', '    }\n', '    // get number of weights in the weights array\n', '    wa_total = weights.length;\n', '\n', '    return ns_total == wa_total;\n', '  }\n', '\n', '    function unpack_data_groups(int256[] _data_groups, bool is_train_data) private {\n', '    int256[datapoint_size][] memory merged_data_group = new int256[datapoint_size][](_data_groups.length/datapoint_size);\n', '\n', '    for (uint i = 0; i < _data_groups.length/datapoint_size; i++) {\n', '      for (uint j = 0; j < datapoint_size; j++) {\n', '        merged_data_group[i][j] = _data_groups[i*datapoint_size + j];\n', '      }\n', '    }\n', '    if (is_train_data == true) {\n', '      // Assign training data\n', '      for (uint k = 0; k < merged_data_group.length; k++) {\n', '        train_data.push(merged_data_group[k]);\n', '      }\n', '    } else {\n', '      // Assign testing data\n', '      for (uint l = 0; l < merged_data_group.length; l++) {\n', '        test_data.push(merged_data_group[l]);\n', '      }\n', '    }\n', '  }\n', '\n', '    function sha_data_group(int256[] data_group, int256 data_group_nonce) private pure returns (bytes32) {\n', '      // Extract the relevant data points for the given data group index\n', '      // We concat all data groups and add the nounce to the end of the array\n', '      // and get the sha256 for the array\n', '      uint index_tracker = 0;\n', '      uint256 total_size = datapoint_size * partition_size;\n', '      /* uint256 start_index = data_group_index * total_size;\n', '      uint256 iter_limit = start_index + total_size; */\n', '      int256[] memory all_data_points = new int256[](total_size+1);\n', '\n', '      for (uint256 i = 0; i < total_size; i++) {\n', '        all_data_points[index_tracker] = data_group[i];\n', '        index_tracker += 1;\n', '      }\n', '      // Add nonce to the whole array\n', '      all_data_points[index_tracker] = data_group_nonce;\n', '      // Return sha256 on all data points + nonce\n', '      return sha256(all_data_points);\n', '    }\n', '\n', '  function relu_activation(int256 x) private pure returns (int256) {\n', '    if (x < 0) {\n', '      return 0;\n', '    } else {\n', '      return x;\n', '    }\n', '  }\n', '\n', '  function get_layer(uint nn) private pure returns (int256[]) {\n', '    int256[] memory input_layer = new int256[](nn);\n', '    return input_layer;\n', '  }\n', '\n', '  function get_hidden_layers(uint[] l_nn) private pure returns (int256[]) {\n', '    uint total_nn = 0;\n', '    // Skip first and last layer since they&#39;re not hidden layers\n', '    for (uint i = 1; i < l_nn.length-1; i++) {\n', '      total_nn += l_nn[i];\n', '    }\n', '    int256[] memory hidden_layers = new int256[](total_nn);\n', '    return hidden_layers;\n', '  }\n', '\n', '  function access_hidden_layer(int256[] hls, uint[] l_nn, uint index) private pure returns (int256[]) {\n', '    // TODO: Bug is here, doesn&#39;t work for between last hidden and output layer\n', '    // Returns the hidden layer from the hidden layers array\n', '    int256[] memory hidden_layer = new int256[](l_nn[index+1]);\n', '    uint hidden_layer_index = 0;\n', '    uint start = 0;\n', '    uint end = 0;\n', '    for (uint i = 0; i < index; i++) {\n', '      start += l_nn[i+1];\n', '    }\n', '    for (uint j = 0; j < (index + 1); j++) {\n', '      end += l_nn[j+1];\n', '    }\n', '    for (uint h_i = start; h_i < end; h_i++) {\n', '      hidden_layer[hidden_layer_index] = hls[h_i];\n', '      hidden_layer_index += 1;\n', '    }\n', '    return hidden_layer;\n', '  }\n', '\n', '  function get_prediction(Submission sub, int[datapoint_size] data_point) private pure returns(int256[]) {\n', '    uint[] memory l_nn = new uint[](sub.num_neurons_hidden_layer.length + 2);\n', '    l_nn[0] = sub.num_neurons_input_layer;\n', '    for (uint i = 0; i < sub.num_neurons_hidden_layer.length; i++) {\n', '      l_nn[i+1] = sub.num_neurons_hidden_layer[i];\n', '    }\n', '    l_nn[sub.num_neurons_hidden_layer.length+1] = sub.num_neurons_output_layer;\n', '    return forward_pass(data_point, sub.weights, sub.biases, l_nn);\n', '  }\n', '\n', '  function forward_pass(int[datapoint_size] data_point, int256[] weights, int256[] biases, uint[] l_nn) private pure returns (int256[]) {\n', '    // Initialize neuron arrays\n', '    int256[] memory input_layer = get_layer(l_nn[0]);\n', '    int256[] memory hidden_layers = get_hidden_layers(l_nn);\n', '    int256[] memory output_layer = get_layer(l_nn[l_nn.length-1]);\n', '\n', '    // load inputs from input layer\n', '    for (uint input_i = 0; input_i < l_nn[0]; input_i++) {\n', '      input_layer[input_i] = data_point[input_i];\n', '    }\n', '    return forward_pass2(l_nn, input_layer, hidden_layers, output_layer, weights, biases);\n', '  }\n', '\n', '  function forward_pass2(uint[] l_nn, int256[] input_layer, int256[] hidden_layers, int256[] output_layer, int256[] weights, int256[] biases) public pure returns (int256[]) {\n', '    // index_counter[0] is weight index\n', '    // index_counter[1] is hidden_layer_index\n', '    uint[] memory index_counter = new uint[](2);\n', '    for (uint layer_i = 0; layer_i < (l_nn.length-1); layer_i++) {\n', '      int256[] memory current_layer;\n', '      int256[] memory prev_layer;\n', '      // If between input and first hidden layer\n', '      if (hidden_layers.length != 0) {\n', '        if (layer_i == 0) {\n', '          current_layer = access_hidden_layer(hidden_layers, l_nn, layer_i);\n', '          prev_layer = input_layer;\n', '        // If between output and last hidden layer\n', '        } else if (layer_i == (l_nn.length-2)) {\n', '          current_layer = output_layer;\n', '          prev_layer = access_hidden_layer(hidden_layers, l_nn, (layer_i-1));\n', '        // If between hidden layers\n', '        } else {\n', '          current_layer = access_hidden_layer(hidden_layers, l_nn, layer_i);\n', '          prev_layer = access_hidden_layer(hidden_layers, l_nn, layer_i-1);\n', '        }\n', '      } else {\n', '        current_layer = output_layer;\n', '        prev_layer = input_layer;\n', '      }\n', '      for (uint layer_neuron_i = 0; layer_neuron_i < current_layer.length; layer_neuron_i++) {\n', '        int total = 0;\n', '        for (uint prev_layer_neuron_i = 0; prev_layer_neuron_i < prev_layer.length; prev_layer_neuron_i++) {\n', '          total += prev_layer[prev_layer_neuron_i] * weights[index_counter[0]];\n', '          index_counter[0]++;\n', '        }\n', '        total += biases[layer_i];\n', '        total = total / int_precision; // Divide by int_precision to scale down\n', '        // If between output and last hidden layer\n', '        if (layer_i == (l_nn.length-2)) {\n', '            output_layer[layer_neuron_i] = relu_activation(total);\n', '        } else {\n', '            hidden_layers[index_counter[1]] = relu_activation(total);\n', '        }\n', '        index_counter[1]++;\n', '      }\n', '    }\n', '    return output_layer;\n', '  }\n', '\n', '  // Fallback function for sending ether to this contract\n', '  function () public payable {}\n', '}']